{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41c63f83",
   "metadata": {
    "id": "41c63f83"
   },
   "source": [
    "# 02b - Recordfile Live Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "hMYuIGFLTxvN",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hMYuIGFLTxvN",
    "outputId": "52c68183-f584-40d4-9bbb-4b777b46feaf"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install --upgrade pip -q\n",
    "!{sys.executable} -m pip install pandas -q\n",
    "!{sys.executable}  -m pip install numpy -q\n",
    "!{sys.executable}  -m pip install matplotlib -q\n",
    "!{sys.executable} -m pip install mgz -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d294dfd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 337
    },
    "id": "6d294dfd",
    "outputId": "0d9e7486-ca36-40cb-bf40-beae2ee6e551"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "from mgz import header, body\n",
    "from mgz.model import parse_match\n",
    "import pickle\n",
    "import os\n",
    "import zipfile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85efbecd-71ef-4c7b-ab1c-9c395401bfc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_txt(msg: str):\n",
    "    #print(msg, end = '\\r')\n",
    "    text_file = open(\"data/scraped_matches/Output_Preprocessing.txt\", \"w\")\n",
    "    text_file.write(msg)\n",
    "    text_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6337c8b4-4f0f-4b2c-8bf3-264f4f07c924",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matches:  225398  - Parseable FNs:  225398\n"
     ]
    }
   ],
   "source": [
    "#settings\n",
    "sizelimit = 999999999999\n",
    "\n",
    "##directorys\n",
    "input_path = 'data/scraped_matches/inputs/'\n",
    "gaia_path = 'data/scraped_matches/gaia_data/'\n",
    "\n",
    "zip_path = \"data/scraped_matches/zipped_recordfiles/\"\n",
    "unzip_path = \"data/scraped_matches/unzipped_recordfiles/\"\n",
    "\n",
    "base_directory = \"data/scraped_matches/\"\n",
    "input_path = base_directory + \"inputs/\"\n",
    "\n",
    "\n",
    "#load parseable recordfiles\n",
    "with open('data/scraped_matches/unzipper/parseable_fns.pkl', 'rb') as f:\n",
    "    parseable_fns = pickle.load(f)\n",
    "    \n",
    "\n",
    "####limit size for testing\n",
    "parseable_fns = parseable_fns[:sizelimit]\n",
    "\n",
    "match_ids = []\n",
    "remove_fns = []\n",
    "matches = {}\n",
    "\n",
    "for fn in parseable_fns:\n",
    "    match_id = fn.partition(\"=\")[2].partition(\"&\")[0]\n",
    "    \n",
    "    if match_id not in match_ids:\n",
    "        match_ids.append(match_id)\n",
    "    else:\n",
    "        remove_fns.append(fn)\n",
    "    \n",
    "    #create dict for each match\n",
    "    matches[match_id] = {}\n",
    "    matches[match_id]['match_id'] = match_id\n",
    "    matches[match_id]['fn'] = fn\n",
    "\n",
    "#remove redundant files\n",
    "for fn in remove_fns:\n",
    "    parseable_fns.remove(fn)\n",
    "\n",
    "print(\"Matches: \" ,len(matches), \" - Parseable FNs: \", len(parseable_fns))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc2618f",
   "metadata": {
    "id": "6dc2618f"
   },
   "source": [
    "**Data Preparation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9fd2471e-79e2-490d-8faa-43b9c9293cda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "163812  found as pkl\n"
     ]
    }
   ],
   "source": [
    "#load already prepared matches - to clear reset pkl_matches after\n",
    "pkl_matches = []\n",
    "\n",
    "for file in os.listdir(os.fsencode(input_path)):\n",
    "        filename = os.fsdecode(file)\n",
    "        match_id = filename[0:-4]\n",
    "        pkl_matches.append(match_id)\n",
    "\n",
    "        \n",
    "#load player masterdata\n",
    "with open(\"masterdata/player_elo.pkl\", 'rb') as f:\n",
    "    player_elo = pickle.load(f)\n",
    "    \n",
    "\n",
    "print(len(pkl_matches), \" found as pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "k5HW5FR-XhrS",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k5HW5FR-XhrS",
    "outputId": "ec82f91c-9488-47b2-a997-eed32653635b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed 4422/225398 - error on: 17 - elo error: 0\r"
     ]
    }
   ],
   "source": [
    "#read in data from record files\n",
    "\n",
    "\n",
    "## orga vars\n",
    "error_count = 0\n",
    "parsed_files = 0\n",
    "elo_error = 0\n",
    "to_be_deleted = []\n",
    "input_dict = {}\n",
    "gaia_dict = {}\n",
    "\n",
    "## content vars for feature creation\n",
    "masterdata_dict = {}\n",
    "\n",
    "\n",
    "masterdata_dict['Build'] = []\n",
    "masterdata_dict['Queue'] = []\n",
    "masterdata_dict['Research'] = []\n",
    "\n",
    "\n",
    "for match_index,match_id in enumerate(matches):\n",
    "    try:\n",
    "        #unzip and recordfile into unzip folder\n",
    "        with zipfile.ZipFile(zip_path + \"{}\".format(matches[match_id]['fn']),\"r\") as zip_ref:\n",
    "            zip_ref.extractall(unzip_path)\n",
    "        \n",
    "        #create recordfilename\n",
    "        record_fn = 'AgeIIDE_Replay_{}.aoe2record'.format(match_id)\n",
    "        \n",
    "        #parse match data\n",
    "        with open(unzip_path + record_fn, 'rb') as data:\n",
    "                eof = os.fstat(data.fileno()).st_size\n",
    "                body.meta.parse_stream(data)\n",
    "                while data.tell() < eof:\n",
    "                    body.operation.parse_stream(data)\n",
    "\n",
    "        with open(unzip_path + record_fn, 'rb') as data:\n",
    "                        match = parse_match(data)\n",
    "\n",
    "        #get general game info  \n",
    "        matches[match_id]['map'] = match.map.name\n",
    "        matches[match_id]['map_size'] = match.map.size\n",
    "        matches[match_id]['duration'] = match.duration.seconds\n",
    "        matches[match_id]['dataset'] = match.dataset\n",
    "        matches[match_id]['difficulty'] = match.difficulty\n",
    "        \n",
    "        matches[match_id]['input_fn'] = input_path + str(match_id) + \".pkl\"\n",
    "        matches[match_id]['gaia_fn'] = gaia_path + str(match_id) + \".pkl\"\n",
    "        \n",
    "        #gather player information\n",
    "        players = match.players\n",
    "        matches[match_id]['p1_name'] = players[0].name\n",
    "        matches[match_id]['p2_name'] = players[1].name\n",
    "        \n",
    "        #get player ELO from dict, replace with oponent value if missing\n",
    "        try:\n",
    "            matches[match_id]['p1_elo'] = player_elo[players[0].profile_id]\n",
    "            matches[match_id]['p2_elo'] = player_elo[players[1].profile_id]\n",
    "        except Exception as e:\n",
    "            try:\n",
    "                matches[match_id]['p1_elo'] = player_elo[players[0].profile_id]\n",
    "                matches[match_id]['p2_elo'] = player_elo[players[0].profile_id]\n",
    "            except:\n",
    "                matches[match_id]['p1_elo'] = player_elo[players[1].profile_id]\n",
    "                matches[match_id]['p2_elo'] = player_elo[players[1].profile_id]\n",
    "            \n",
    "            \n",
    "        matches[match_id]['p1_civ'] = players[0].civilization\n",
    "        matches[match_id]['p2_civ'] = players[1].civilization\n",
    "\n",
    "        matches[match_id]['p1_xpos'] = players[0].position.x\n",
    "        matches[match_id]['p2_xpos'] = players[1].position.x\n",
    "        matches[match_id]['p1_ypos'] = players[0].position.y\n",
    "        matches[match_id]['p2_ypos'] = players[1].position.y\n",
    "        \n",
    "        \n",
    "\n",
    "        if players[0].winner == True:\n",
    "            matches[match_id]['winner'] = int(0)\n",
    "        else:\n",
    "            matches[match_id]['winner'] = int(1)\n",
    "\n",
    "        #read inputs and gaia_data if not already in pkl\n",
    "        if match_id not in pkl_matches: \n",
    "\n",
    "            #get all item names\n",
    "            for object in match.inputs:\n",
    "                attr_list = object.__dict__.keys()\n",
    "                break\n",
    "\n",
    "            if not str(getattr(object,'type')) == \"Chat\":\n",
    "                inputs_objects_dict = {}\n",
    "\n",
    "            for index, object in enumerate(match.inputs):\n",
    "                inputs_objects_dict[index] = {}\n",
    "\n",
    "\n",
    "                #create timestamp_seconds\n",
    "                inputs_objects_dict[index]['ts_seconds'] = getattr(object,'timestamp').seconds\n",
    "\n",
    "                for attribute in attr_list:\n",
    "                    inputs_objects_dict[index][attribute] = str(getattr(object,attribute))\n",
    "\n",
    "\n",
    "                    #create a list of Builds\n",
    "                    if str(getattr(object,'type')) == 'Build':\n",
    "                        val = str(getattr(object,'param'))\n",
    "                        if val not in masterdata_dict['Build'] :\n",
    "                            masterdata_dict['Build'] .append(val)\n",
    "\n",
    "                    #create list of possible Units\n",
    "                    if str(getattr(object,'type')) == 'Queue':\n",
    "                        val = str(getattr(object,'param'))\n",
    "                        if val not in masterdata_dict['Queue'] :\n",
    "                            masterdata_dict['Queue'] .append(val)\n",
    "\n",
    "                    #create list of Technologies\n",
    "                    if str(getattr(object,'type')) == 'Research':\n",
    "                        val = str(getattr(object,'param'))\n",
    "                        if val not in masterdata_dict['Research'] :\n",
    "                            masterdata_dict['Research'].append(val)\n",
    "\n",
    "\n",
    "                    #create redundant single xy coordinates\n",
    "                    if attribute == 'position':\n",
    "                        pos = getattr(object,attribute)\n",
    "\n",
    "                        if pos == None:\n",
    "                            inputs_objects_dict[index]['x_pos']= None\n",
    "                            inputs_objects_dict[index]['y_pos'] = None \n",
    "                        else:\n",
    "                            inputs_objects_dict[index][\"x_pos\"] = getattr(object,attribute).x\n",
    "                            inputs_objects_dict[index][\"y_pos\"] = getattr(object,attribute).y\n",
    "\n",
    "                    #replace playerinfo\n",
    "                    if attribute == 'player':\n",
    "                        if inputs_objects_dict[index][attribute] == matches[match_id]['p1_name']:\n",
    "                            inputs_objects_dict[index][attribute] = 'p1'\n",
    "                        else:\n",
    "                            inputs_objects_dict[index][attribute] = 'p2'\n",
    "\n",
    "\n",
    "\n",
    "            #read gaia data\n",
    "            gaia_objects_dict = {}\n",
    "            for object in match.gaia:\n",
    "                if getattr(object,\"name\"):\n",
    "                    #read gaia data in\n",
    "                    gaia_objects_dict[object.instance_id] = {}\n",
    "                    gaia_objects_dict[object.instance_id][\"name\"] = getattr(object,\"name\")\n",
    "                    gaia_objects_dict[object.instance_id][\"x_pos\"] = getattr(object,'position').x\n",
    "                    gaia_objects_dict[object.instance_id][\"y_pos\"] = getattr(object,'position').y\n",
    "\n",
    "\n",
    "            #persistate inputs in pkl file\n",
    "            output = open(matches[match_id]['input_fn'] , 'wb')\n",
    "            pickle.dump(inputs_objects_dict, output)\n",
    "            output.close()\n",
    "\n",
    "            #persitate gaia_data\n",
    "            output = open(matches[match_id]['gaia_fn'] , 'wb')\n",
    "            pickle.dump(gaia_objects_dict, output)\n",
    "            output.close()\n",
    "            \n",
    "            os.remove(unzip_path + record_fn)\n",
    "\n",
    "    except Exception as e:\n",
    "        #print(e, \"Match_id:\", match_id)\n",
    "        to_be_deleted.append(match_id)\n",
    "        error_count +=1\n",
    "        if error_count >= 100:\n",
    "          pass#break\n",
    "        pass\n",
    " \n",
    "    parsed_files += 1\n",
    "    print(f'Parsed {parsed_files}/{len(matches)} - error on: {error_count} - elo error: {elo_error}',  end='\\r')\n",
    "\n",
    "    if match_index % 1000 == 0:\n",
    "        #print and persistate\n",
    "        print_txt(f'Parsed: {parsed_files}/{len(matches)} - error on: {error_count} - elo error: {elo_error}')\n",
    "        #persitate matches\n",
    "        file_name = base_directory + f'parsed_matches/over_time/{match_index}_matches.pkl'\n",
    "        output = open(file_name, 'wb')\n",
    "        pickle.dump(matches, output)\n",
    "        output.close()\n",
    "    \n",
    "    \n",
    "#errorhandling\n",
    "print(\"Parsed:\" , parsed_files, \"- error on: \", error_count)\n",
    "#print(\"Deleting: \", to_be_deleted)\n",
    "for match_id in to_be_deleted:\n",
    "    del matches[match_id]\n",
    "\n",
    "print(\"Error on \",error_count,\" matches while parsing recordfile data\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7d7990-e1d4-4d14-8877-f012b384da0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#final persitate matches\n",
    "file_name = 'data/scraped_matches/parsed_matches/final_matches.pkl'\n",
    "output = open(file_name, 'wb')\n",
    "pickle.dump(matches, output)\n",
    "output.close()\n",
    "\n",
    "matches_df = pd.DataFrame.from_dict(matches, orient='index')\n",
    "\n",
    "#final persitate matches df\n",
    "file_name = 'data/scraped_recordfile_matches.csv'\n",
    "output = open(file_name, 'wb')\n",
    "pickle.dump(matches_df, output)\n",
    "output.close()\n",
    "\n",
    "matches_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32b4de6-c5ec-4521-8ca0-205b782439d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#manual add masterdata\n",
    "masterdata_dict['Ages'] = ['Feudal Age','Castle Age','Imperial Age']\n",
    "\n",
    "for key in masterdata_dict:\n",
    "    print(key, \": \",len(masterdata_dict[key]))\n",
    "\n",
    "#persistate master data\n",
    "masterdata_path = 'masterdata/'\n",
    "\n",
    "output = open(masterdata_path + 'masterdata_dict.pkl' , 'wb')\n",
    "pickle.dump(masterdata_dict, output)\n",
    "output.close()\n",
    "\n",
    "masterdata_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80effda6-6384-436a-8fff-8f1c74bc4c57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80f6491-2f35-429b-aea8-777332a88eaa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "name": "win_prob_calculation-checkpoint.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "aoeanalytics2",
   "language": "python",
   "name": "aoeanalytics2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
