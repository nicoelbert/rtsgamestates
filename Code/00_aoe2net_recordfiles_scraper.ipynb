{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "wUixTUMzvJMs",
   "metadata": {
    "id": "wUixTUMzvJMs"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/niel\n"
     ]
    }
   ],
   "source": [
    "cd ..\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ccb191b-f4d2-4bb1-99a9-cddd22f934f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install requests --q\n",
    "!{sys.executable} -m pip install bs4 --q\n",
    "!{sys.executable} -m pip install numpy --q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d294dfd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 337
    },
    "id": "6d294dfd",
    "outputId": "1cb0e963-5c7c-4014-d1fd-a3142ca2e94d"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json  \n",
    "import pickle\n",
    "import requests\n",
    "import lxml\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import zipfile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d2b269c-b910-4a1f-ae3c-445edfba35dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34maoe_data\u001b[0m/     \u001b[01;34mDokumentenanalyse\u001b[0m/  \u001b[01;34mfastai_nbdev_test\u001b[0m/  Untitled.ipynb\n",
      "aoe_data.zip  \u001b[01;34mdrive\u001b[0m/              \u001b[01;34mnbdev\u001b[0m/              \u001b[01;34mzara\u001b[0m/\n",
      "\u001b[01;34mdeepflash2\u001b[0m/   \u001b[01;34mdrive_files\u001b[0m/        Untitled1.ipynb\n"
     ]
    }
   ],
   "source": [
    "ls\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc2618f",
   "metadata": {
    "id": "6dc2618f"
   },
   "source": [
    "**Data and Settings**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "daafa83f-0855-4189-827a-014b6725206e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#settings\n",
    "scrape_player_names = True\n",
    "scrape_matches= True\n",
    "parallel_download = True\n",
    "\n",
    "\n",
    "#directorys\n",
    "player_file_path = \"aoe_data/data/scraped_matches/players.pkl\"\n",
    "player_elo_path = \"aoe_data/masterdata/player_elo.pkl\"\n",
    "\n",
    "found_matches_filepath = \"aoe_data/data/scraped_matches/found_recordfile_matches.pkl\"\n",
    "\n",
    "zip_path = \"aoe_data/data/scraped_matches/zipped_recordfiles/\"\n",
    "unzip_path = \"aoe_data/data/scraped_matches/unzipped_recordfiles/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9814e39a-732f-43e9-b9b4-fb82fa790e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#find already scraped matches for parallel download\n",
    "scraped_matches = []\n",
    "\n",
    "for file in os.listdir(zip_path):\n",
    "    filename = os.fsdecode(file)\n",
    "\n",
    "    match_id = filename.partition(\"=\")[2].partition(\"&\")[0]\n",
    "    scraped_matches.append(match_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c9d315-3974-4c0b-9fcb-26f3f8aa2c51",
   "metadata": {},
   "source": [
    "**Functions**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "v1kHivkVv6Zm",
   "metadata": {
    "id": "v1kHivkVv6Zm"
   },
   "outputs": [],
   "source": [
    "def scrape_gameinfo_by_playername(p1_name: str,p2_name: str):\n",
    "    #search for p1 name on aoe2insights\n",
    "    source = requests.get('https://www.aoe2insights.com/search/?q={}'.format(p1_name)).text\n",
    "    soup = BeautifulSoup(source, 'html.parser')\n",
    "\n",
    "    user_string = soup.find('a', class_='stretched-link')#.__dict__.keys()\n",
    "    p1_id = user_string['href'].split('/')[2]\n",
    "\n",
    "    #search for RM 1v1 matches with p2\n",
    "    source = requests.get('https://www.aoe2insights.com/user/{}/matches/?ladder=3&player={}'.format(p1_id,p2_name)).text\n",
    "    soup = BeautifulSoup(source, 'html.parser')\n",
    "\n",
    "    match_string = soup.find('a', class_='float-right stretched-link')#.__dict__.keys()\n",
    "    match_id = match_string['href'].split('/')[2]\n",
    "    \n",
    "    return {'p1_id': p1_id, 'match_id': match_id}\n",
    "\n",
    "def scrape_gameinfo_by_playerid(p1_id: int,p2_id: int):\n",
    "    #search for RM 1v1 matches with p2\n",
    "    source = requests.get('https://www.aoe2insights.com/user/{}/matches/?ladder=3&player={}'.format(p1_id,p2_name)).text\n",
    "    soup = BeautifulSoup(source, 'html.parser')\n",
    "\n",
    "    match_string = soup.find('a', class_='float-right stretched-link')#.__dict__.keys()\n",
    "    match_id = match_string['href'].split('/')[2]\n",
    "    \n",
    "    return {'p1_id': p1_id, 'match_id': match_id}\n",
    "\n",
    "\n",
    "def download_file(url, save_path):\n",
    "    local_filename = save_path + url.split('/')[-1]\n",
    "    # NOTE the stream=True parameter below\n",
    "    with requests.get(url, stream=True) as r:\n",
    "        r.raise_for_status()\n",
    "        with open(local_filename, 'wb') as f:\n",
    "            for chunk in r.iter_content(chunk_size=8192): \n",
    "                # If you have chunk encoded response uncomment if\n",
    "                # and set chunk_size parameter to None.\n",
    "                #if chunk: \n",
    "                f.write(chunk)\n",
    "\n",
    "    return local_filename\n",
    "\n",
    "\n",
    "def download_match(match_id, pA_id, pB_id, zip_path):\n",
    "    #check if two users were passed:\n",
    "    if pA_id == pB_id:\n",
    "        two_user = False\n",
    "    else: two_user = True\n",
    "    \n",
    "    #create url\n",
    "    url = \"https://aoe.ms/replay/?gameId={}&profileId={}\".format(str(match_id),str(pA_id))\n",
    "    response = requests.get(url).status_code\n",
    "\n",
    "    if response == 404:\n",
    "        if two_user:\n",
    "            new_url = \"https://aoe.ms/replay/?gameId={}&profileId={}\".format(str(match_id),str(pB_id))\n",
    "            response = requests.get(url).status_code\n",
    "\n",
    "            if response == 404:\n",
    "                found = False\n",
    "            else:\n",
    "                try:\n",
    "                    download_file(new_url, zip_path)\n",
    "                    found = True\n",
    "                except:\n",
    "                    found = False\n",
    "        else: found = False\n",
    "    else:\n",
    "        try:\n",
    "            download_file(url, zip_path)\n",
    "            found = True\n",
    "        except:\n",
    "            found = False\n",
    "        \n",
    "    return found, response\n",
    "\n",
    "\n",
    "def print_txt(msg: str):\n",
    "    print(msg, end = '\\r')\n",
    "    text_file = open(\"aoe_data/Outputs/Download_Output.txt\", \"w\")\n",
    "    text_file.write(msg)\n",
    "    text_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddbeae34-c4c8-40ee-9dbe-c3cb8c66b02f",
   "metadata": {},
   "source": [
    "**Scrape**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "60941415-f89f-4683-83ad-35c6f8ba7014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39906  Players found. error on:  1322 - Found Players: 3990639906- Found Players: 399061493  Pages - error on:  1093 - Found Players: 39906\n"
     ]
    }
   ],
   "source": [
    "player_elos = {}\n",
    "\n",
    "\n",
    "#scrape player names\n",
    "if scrape_player_names:\n",
    "    players = {}\n",
    "    errors = 0\n",
    "    page_number = 1\n",
    "    max_page_number = 1723\n",
    "\n",
    "    while page_number < max_page_number:\n",
    "        try:\n",
    "            #scrape aoe2insights\n",
    "            source = requests.get('https://www.aoe2insights.com/leaderboard/3/?page={}'.format(str(page_number))).text\n",
    "            soup = BeautifulSoup(source, 'html.parser')\n",
    "            table = soup.find('table','table table-striped table-dark table-responsive-sm')\n",
    "\n",
    "            for row in table.findAll(\"tr\"):\n",
    "                #get user id\n",
    "                cells = row.findAll(\"a\")\n",
    "                for e in cells:\n",
    "                    if \"/user/\" in str(e['href']):\n",
    "                        user_id = e['href'].replace('/user/','').replace('/','')\n",
    "                        username = e.string.strip()\n",
    "                        players[user_id] = username\n",
    "                \n",
    "                #get user rating\n",
    "                cells = row.findAll(\"td\")\n",
    "                for e in cells: \n",
    "                    elem = e.next_element.strip()\n",
    "                    if elem.isnumeric():\n",
    "                        player_elos[int(user_id)] = int(elem)\n",
    "            #persistate\n",
    "            if page_number % 50 == 0:\n",
    "                output = open(player_file_path , 'wb')\n",
    "                pickle.dump(players, output)\n",
    "                output.close()\n",
    "        except:\n",
    "            errors +=1\n",
    "\n",
    "        print(\"Parsed:\" , page_number, \" Pages - error on: \", errors,\"- Found Players:\",len(players),  end='\\r')\n",
    "        page_number +=1\n",
    "\n",
    "    #final persistation\n",
    "    output = open(player_file_path , 'wb')\n",
    "    pickle.dump(players, output)\n",
    "    output.close()\n",
    "\n",
    "    #final persistation\n",
    "    output = open(player_elo_path , 'wb')\n",
    "    pickle.dump(player_elos, output)\n",
    "    output.close()\n",
    "        \n",
    "else:\n",
    "    with open(player_file_path, 'rb') as f:\n",
    "        players = pickle.load(f)\n",
    "    \n",
    "print(len(players), \" Players found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "64b039ee-05c2-458e-87b3-53e948bf2ebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Download\r"
     ]
    }
   ],
   "source": [
    "   \n",
    "start_msg = \"Start Download\"\n",
    "print_txt(start_msg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6abacf53-1757-4f22-b125-6db1f2a7ff81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed: 119  Players - error on:  0 - Found Matches: 1055  - Found Files:  79  - Failed Files:  301  - Last Response:  404sponse [200]>\r"
     ]
    }
   ],
   "source": [
    "matches = {}\n",
    "errors = 0\n",
    "max_pages = 1\n",
    "directory = \"aoe_data/data/scraped_matches/\"\n",
    "file_path_matches = directory + \"matches.pkl\"\n",
    "file_path_scraped_players = directory +\"already_scraped_players.pkl\"\n",
    "scraped_players = []\n",
    "scraped_files = 0\n",
    "failed_files = 0\n",
    "response = \"\"\n",
    "\n",
    "if scrape_matches:\n",
    "    for index, user_id in enumerate(players):\n",
    "        try:\n",
    "            #reset parameter\n",
    "            page_number = 1\n",
    "            \n",
    "            #only scrape first n pages of games\n",
    "            while page_number<=max_pages:\n",
    "                #make request\n",
    "                response = requests.get(\"https://www.aoe2insights.com/user/{}/matches/?ladder=3&page={}\".format(str(user_id),str(page_number)))\n",
    "                source = response.text\n",
    "                soup = BeautifulSoup(source, 'html.parser')\n",
    "\n",
    "                #check if page has matches on it\n",
    "                warning = soup.find('div',class_ ='col-12 alert alert-warning')\n",
    "                if warning is not None or response.status_code == 404:\n",
    "                    break\n",
    "\n",
    "                #get match_id\n",
    "                match_links = soup.findAll('a',class_='float-right stretched-link')\n",
    "                for e in match_links:\n",
    "                    match_id = e['href'].replace('/match/','').replace('/','')\n",
    "\n",
    "                    #fill data\n",
    "                    if match_id in matches:\n",
    "                        matches[match_id]['pB_id'] = user_id\n",
    "                        \n",
    "                        if parallel_download:\n",
    "                            #try to scrape recordfiles when full info is gathered\n",
    "                            if match_id not in scraped_matches:\n",
    "                                try:\n",
    "                                    found, response = download_match(match_id, matches[match_id]['pA_id'], matches[match_id]['pB_id'], zip_path)\n",
    "                                    if found:\n",
    "                                        scraped_files +=1\n",
    "                                        #reduce fails bc it was counted on first attempt\n",
    "                                        failed_files -= 1\n",
    "                                    else:\n",
    "                                        failed_files += 1\n",
    "                                except:\n",
    "                                    failed_files +=1\n",
    "                    else:\n",
    "                        matches[match_id] = {}\n",
    "                        matches[match_id]['match_id'] = match_id \n",
    "                        matches[match_id]['pA_id'] = user_id\n",
    "                        matches[match_id]['pB_id'] = None\n",
    "                        \n",
    "                        if parallel_download:\n",
    "                            if match_id not in scraped_matches:\n",
    "                                    try:\n",
    "                                        found, response = download_match(match_id, matches[match_id]['pA_id'], matches[match_id]['pA_id'], zip_path)\n",
    "                                        if found:\n",
    "                                            scraped_files +=1\n",
    "                                        else:\n",
    "                                            failed_files += 1\n",
    "                                    except:\n",
    "                                        failed_files +=1\n",
    "                    \n",
    "                    if user_id not in scraped_players:\n",
    "                        scraped_players.append(user_id)\n",
    "                    \n",
    "                    #persistate\n",
    "                    if len(matches) % 1000 == 0:\n",
    "                        output = open(directory + \"over_time/\" + str(len(matches))+ \"_matches.pkl\" , 'wb')\n",
    "                        pickle.dump(matches, output)\n",
    "                        output.close()\n",
    "                        \n",
    "                        output = open(directory + \"over_time/\" + str(len(matches))+ \"_already_scraped_players.pkl\" , 'wb')\n",
    "                        pickle.dump(scraped_players, output)\n",
    "                        output.close()\n",
    "                        \n",
    "                        print_txt(\"Parsed:\" + str(index) + \" Players - error on: \" + str(errors) +\"- Found Matches:\"+ str(len(matches)) + \" - Found Files: \" + str(scraped_files)+  \" - Failed Files: \" +str(failed_files))\n",
    "   \n",
    "                print(\"Parsed:\" , index, \" Players - error on: \", errors,\"- Found Matches:\",len(matches), \" - Found Files: \",str(scraped_files), \" - Failed Files: \",str(failed_files), \" - Last Response: \",response,end='\\r')\n",
    "                page_number +=1\n",
    "                \n",
    "        except:\n",
    "            errors +=1\n",
    "            print(\"Parsed:\" , index, \" Players - error on: \", errors,\"- Found Matches:\",len(matches), \" - Found Files: \",str(scraped_files), \" - Failed Files: \",str(failed_files),  end='\\r')\n",
    "            pass\n",
    "    \n",
    "    #final persistation\n",
    "    output = open(file_path_matches , 'wb')\n",
    "    pickle.dump(matches, output)\n",
    "    output.close()\n",
    "    \n",
    "else:\n",
    "    with open(file_path_matches, 'rb') as f:\n",
    "        matches = pickle.load(f)\n",
    "\n",
    "print(len(matches), \" Matches Found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9plTTXb7uxMv",
   "metadata": {
    "id": "9plTTXb7uxMv"
   },
   "outputs": [],
   "source": [
    "matches_df = pd.DataFrame.from_dict(matches, orient='index')\n",
    "print(matches_df.shape)\n",
    "matches_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "VzuI2mBlv1OG",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VzuI2mBlv1OG",
    "outputId": "9564ab0c-0b81-4ab7-a778-5457c8bb4b55"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314a988f-0c1d-4f39-8569-7d953a6b0c42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b88a54d-e497-443a-8b41-322a38f5491e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04780007-11bb-456e-9154-9b87f5b6b3c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "name": "win_prob_calculation-checkpoint.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "aoeanalytics2",
   "language": "python",
   "name": "aoeanalytics2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
